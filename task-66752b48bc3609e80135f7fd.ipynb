{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO7RESF7p7dKDz7bTv8hjct"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Response 1"],"metadata":{"id":"EzbtO2UIVlsg"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, LSTM\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.utils import get_file\n","import numpy as np\n","\n","# Download the dataset\n","path = get_file('alice.txt', origin='https://www.gutenberg.org/files/11/11-0.txt')\n","\n","# Read the text\n","text = open(path).read().lower()\n","\n","# Remove unnecessary characters\n","text = text.replace('\\n', ' ')\n","text = text.replace('\\r', ' ')\n","text = text.replace('\\ufeff', '')\n","\n","# Create a list of unique characters\n","chars = sorted(list(set(text)))\n","\n","# Create dictionaries to map characters to indices and vice versa\n","char_indices = dict((c, i) for i, c in enumerate(chars))\n","indices_char = dict((i, c) for i, c in enumerate(chars))\n","\n","# Define the sequence length and step size\n","maxlen = 40\n","step = 3\n","\n","# Create semi-redundant sequences\n","sentences = []\n","next_chars = []\n","for i in range(0, len(text) - maxlen, step):\n","    sentences.append(text[i: i + maxlen])\n","    next_chars.append(text[i + maxlen])\n","\n","# Create the training data\n","X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool_)\n","y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n","for i, sentence in enumerate(sentences):\n","    for t, char in enumerate(sentence):\n","        X[i, t, char_indices[char]] = 1\n","    y[i, char_indices[next_chars[i]]] = 1\n","\n","# Define the model\n","model = Sequential()\n","model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n","model.add(Dense(len(chars)))\n","model.add(Activation('softmax'))\n","\n","# Compile the model\n","optimizer = RMSprop(learning_rate=0.01)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X, y, batch_size=256, epochs=4)\n","\n","# Define a function to sample an index from a probability array\n","def sample(preds, temperature=1.0):\n","    preds = np.asarray(preds).astype(\"float64\")\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)\n","\n","# Generate text\n","diversity = 0.2\n","sentence = 'Alice was beginning to get very tired'\n","generated = sentence\n","print('...Generating with seed: \"' + sentence + '\"')\n","for i in range(100):\n","    x_pred = np.zeros((1, maxlen, len(chars)))\n","    for t, char in enumerate(sentence):\n","        x_pred[0, t, char_indices[char]] = 1.0\n","    preds = model.predict(x_pred, verbose=0)[0]\n","    next_index = sample(preds, diversity)\n","    next_char = indices_char[next_index]\n","    sentence = sentence[1:] + next_char\n","    generated += next_char\n","print(\"...Generated: \", generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"4zBk5pNlwdja","executionInfo":{"status":"error","timestamp":1719341837692,"user_tz":180,"elapsed":15551,"user":{"displayName":"Dami Dória Narayana Duarte","userId":"04065118723336904824"}},"outputId":"122f192a-5fab-47ea-af02-b8ef802fd8e7"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.gutenberg.org/files/11/11-0.txt\n","154638/154638 [==============================] - 1s 4us/step\n","Epoch 1/4\n","189/189 [==============================] - 3s 7ms/step - loss: 2.5037 - accuracy: 0.3086\n","Epoch 2/4\n","189/189 [==============================] - 1s 7ms/step - loss: 1.9511 - accuracy: 0.4406\n","Epoch 3/4\n","189/189 [==============================] - 1s 7ms/step - loss: 1.7434 - accuracy: 0.4900\n","Epoch 4/4\n","189/189 [==============================] - 1s 6ms/step - loss: 1.6070 - accuracy: 0.5268\n","...Generating with seed: \"Alice was beginning to get very tired\"\n"]},{"output_type":"error","ename":"KeyError","evalue":"'A'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-b88f60ee5e69>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mx_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'A'"]}]},{"cell_type":"markdown","source":["# Response 2"],"metadata":{"id":"t_wk8HxcVpKf"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, LSTM\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.utils import get_file\n","import numpy as np\n","import random\n","\n","# Download the dataset\n","path = get_file('alice.txt', origin='https://www.gutenberg.org/files/11/11-0.txt')\n","text = open(path).read().lower()\n","\n","# Remove unnecessary characters\n","text = text.replace('--', ' ')\n","text = text.replace('\\n', ' ')\n","text = text.replace('\"', '')\n","text = text.replace(\"'\", '')\n","\n","# Create a set of unique characters\n","chars = sorted(list(set(text)))\n","char_indices = dict((c, i) for i, c in enumerate(chars))\n","indices_char = dict((i, c) for i, c in enumerate(chars))\n","\n","# Cut the text in semi-redundant sequences of maxlen characters\n","maxlen = 40\n","step = 3\n","sentences = []\n","next_chars = []\n","for i in range(0, len(text) - maxlen, step):\n","    sentences.append(text[i: i + maxlen])\n","    next_chars.append(text[i + maxlen])\n","\n","# One-hot encode the sequences\n","X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool_)\n","y = np.zeros((len(sentences), len(chars)), dtype=np.bool_)\n","for i, sentence in enumerate(sentences):\n","    for t, char in enumerate(sentence):\n","        X[i, t, char_indices[char]] = 1\n","    y[i, char_indices[next_chars[i]]] = 1\n","\n","# Build the model\n","model = Sequential()\n","model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n","model.add(Dense(len(chars)))\n","model.add(Activation('softmax'))\n","optimizer = RMSprop(learning_rate=0.01)\n","model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X, y, batch_size=256, epochs=4)\n","\n","# Define a function to sample an index from a probability array\n","def sample(preds, temperature=1.0):\n","    preds = np.asarray(preds).astype(\"float64\")\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)\n","\n","# Generate text\n","diversity = 0.2\n","sentence = 'Alice was beginning to get very tired'\n","generated = sentence\n","print('...Generating with seed: \"' + sentence + '\"')\n","for i in range(100):\n","    x_pred = np.zeros((1, maxlen, len(chars)))\n","    for t, char in enumerate(sentence):\n","        x_pred[0, t, char_indices[char]] = 1.0\n","    preds = model.predict(x_pred, verbose=0)[0]\n","    next_index = sample(preds, diversity)\n","    next_char = indices_char[next_index]\n","    sentence = sentence[1:] + next_char\n","    generated += next_char\n","print(\"...Generated: \", generated)"],"metadata":{"id":"ujU_6p6vk_uP","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"error","timestamp":1719341853511,"user_tz":180,"elapsed":9140,"user":{"displayName":"Dami Dória Narayana Duarte","userId":"04065118723336904824"}},"outputId":"64935192-784e-4d20-e79d-1f345ddd5c98"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","189/189 [==============================] - 4s 6ms/step - loss: 2.4685 - accuracy: 0.3134\n","Epoch 2/4\n","189/189 [==============================] - 1s 6ms/step - loss: 1.9326 - accuracy: 0.4426\n","Epoch 3/4\n","189/189 [==============================] - 1s 6ms/step - loss: 1.7221 - accuracy: 0.4957\n","Epoch 4/4\n","189/189 [==============================] - 1s 6ms/step - loss: 1.5827 - accuracy: 0.5333\n","...Generating with seed: \"Alice was beginning to get very tired\"\n"]},{"output_type":"error","ename":"KeyError","evalue":"'A'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-a75bb0a697ac>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mx_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'A'"]}]}]}